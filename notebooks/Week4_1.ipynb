{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nbBfI_8PbVDO"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Hyperparams\n",
    "# training config\n",
    "NUM_EPOCHS= 5\n",
    "LR=0.001\n",
    "# dataset config\n",
    "batch_size = 128\n",
    "generator=torch.Generator().manual_seed(42) # Can be included for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgyFDQs2bK7t",
    "outputId": "1a37f3db-c13e-40f1-c1c2-177cb3ee0c27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QuGtFcYBaTwL"
   },
   "outputs": [],
   "source": [
    "_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "NUM_CLASSES = 0\n",
    "\n",
    "def getTrainingSet(dataset_name):\n",
    "  if dataset_name == 'CIFAR-10':\n",
    "    print(\"in cifar-10\")\n",
    "    NUM_CLASSES=10\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                          download=True, transform=transform_train)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                          download=True, transform=transform_test)\n",
    "    \n",
    "\n",
    "    trainset, validset = torch.utils.data.random_split(trainset, \n",
    "                                                      [int(len(trainset)*0.8),len(trainset)- \n",
    "                                                      int(len(trainset)*0.8)], generator=generator)\n",
    "    \n",
    "  elif dataset_name == 'tiny-imagenet':\n",
    "    NUM_CLASSES=200\n",
    "    \n",
    "    !wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "    !unzip -qq 'tiny-imagenet-200.zip'\n",
    "    \n",
    "    \n",
    "    totalset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train', \n",
    "                                                   transform=transform_train)\n",
    "  \n",
    "    train_counts = [0] * 200\n",
    "    valid_counts = [0] * 200\n",
    "    trainset = []\n",
    "    validset = []\n",
    "    testset = []\n",
    "    for item in totalset:\n",
    "        if train_counts[item[1]] < 350:\n",
    "            trainset.append(item)\n",
    "            train_counts[item[1]] += 1\n",
    "        elif valid_counts[item[1]] < 75:\n",
    "            validset.append(item)\n",
    "            valid_counts[item[1]] += 1\n",
    "        else:\n",
    "            testset.append(item)\n",
    "\n",
    "  elif dataset_name == 'Caltech101':\n",
    "    NUM_CLASSES=101\n",
    "    !gdown https://drive.google.com/uc?id=1DX_XeKHn3yXtZ18DD7qc1wf-Jy5lnhD5\n",
    "    !unzip -qq '101_ObjectCategories.zip' \n",
    "\n",
    "    PATH = '101_ObjectCategories/'\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "      [transforms.CenterCrop(256),\n",
    "      transforms.Resize((64,64)),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "    totalset = torchvision.datasets.ImageFolder(PATH, transform=transform_train)\n",
    "\n",
    "    X, y = zip(*totalset)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, \n",
    "                                                      stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, \n",
    "                                                    test_size = 0.5, \n",
    "                                                    stratify=y_val)\n",
    "\n",
    "    trainset, validset, testset = list(zip(X_train, y_train)), list(zip(X_val, y_val)), list(zip(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "  validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size,\n",
    "                                            shuffle=False,num_workers=2)\n",
    "  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "  return trainset, testset, trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGStyleNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10, init_weights: bool = True):\n",
    "        super(VGGStyleNet, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128*8*8, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 nn.init.constant_(m.weight, 1)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1_1(x))\n",
    "        out = F.relu(self.conv1_2(out))\n",
    "        out = self.maxpool1(out)\n",
    "        out = F.relu(self.conv2_1(out))\n",
    "        out = F.relu(self.conv2_2(out))\n",
    "        out = self.maxpool2(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSnzvTFYGxnV"
   },
   "outputs": [],
   "source": [
    "class ResNetBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(ResNetBasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBZ8k0lAG2dL"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kINn6HSGG9y_"
   },
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    NUM_CLASSES=200\n",
    "    return ResNet(ResNetBasicBlock, [2,2,2,2], NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IM8X5tXRHCFR",
    "outputId": "e1877b0e-a420-4186-fb1d-dbeeb84fa699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in cifar-10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "net = VGGStyleNet()\n",
    "net.to(device)\n",
    "trainset, testset, trainloader, testloader = getTrainingSet(\"CIFAR-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9BQiF_TPMXH8"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QFftkVJfMwgB"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  net.train()\n",
    "  correct_images = 0\n",
    "  total_images = 0\n",
    "  training_loss = 0\n",
    "  for batch_index, (images, labels) in enumerate(tqdm(trainloader)):\n",
    "    optimizer.zero_grad()\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = net(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    training_loss += loss.item()\n",
    "    _, predicted = outputs.max(1)\n",
    "    total_images += labels.size(0)\n",
    "    correct_images += predicted.eq(labels).sum().item()\n",
    "  print('Epoch: %d, Loss: %.3f, '\n",
    "              'Accuracy: %.3f%% (%d/%d)' % (epoch, training_loss/(batch_index+1),\n",
    "                                       100.*correct_images/total_images, correct_images, total_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Yfxc3akdUDSi"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    test_loss = 0\n",
    "    total_images = 0\n",
    "    correct_images = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "      for batch_index, (images, labels) in enumerate(tqdm(testloader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_images += labels.size(0)\n",
    "        correct_images += predicted.eq(labels).sum().item()\n",
    "#         print(batch_index, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "#                   % (test_loss/(batch_index+1), 100.*correct_images/total_images, correct_images, total_images))\n",
    "    test_accuracy = 100.*correct_images/total_images\n",
    "    print(\"accuracy of test set is\",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "Qu4r6wHJVmL4",
    "outputId": "c8013a52-b68a-4578-ca75-7a96bcc44d8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/313 [00:00<?, ?it/s]C:\\Users\\Kiran\\anaconda3\\envs\\dl_week4\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "100%|██████████| 313/313 [00:19<00:00, 15.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.179, Accuracy: 19.030% (7612/40000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:18<00:00, 16.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.914, Accuracy: 30.558% (12223/40000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:18<00:00, 16.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 1.747, Accuracy: 36.388% (14555/40000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:18<00:00, 16.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 1.640, Accuracy: 39.792% (15917/40000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:18<00:00, 16.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 1.563, Accuracy: 42.750% (17100/40000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "  train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "qc_eyO6ihGf9",
    "outputId": "5f2b908a-833f-4614-e914-8a07b0a16708"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 27.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of test set is 47.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week4-1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
